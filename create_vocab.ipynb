{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary\n",
    "\n",
    "Create a character vocabulary by reading in abstracts from multiple files. This will be used\n",
    "as a common vocabulary for all RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "abstracts-small.json:\n",
      "1407.0001: The topic of finding effective strategy to halt virus in complex network is of c...\n",
      "1407.0004: Linear precoding exploits the spatial degrees of freedom offered by multi-antenn...\n",
      "1407.0016: We present completed observations of the NGC 7448 galaxy group and background vo...\n",
      "1407.0017: Increasingly stringent limits from LHC searches for new physics, coupled with la...\n",
      "1407.0023: Results are presented for an initial survey of the Norma Arm gathered with the f...\n",
      "1407.0026: Current time-domain wide-field sky surveys generally operate with few-degree-siz...\n",
      "1407.0029: Spinless fermions on a honeycomb lattice provide a minimal realization of lattic...\n",
      "1407.0030: \\vskip 3pt \\noindent The strong CP-violating parameter is small today as indicat...\n",
      "1407.0031: Cross-correlating the Planck High Frequency Instrument (HFI) maps against quasar...\n",
      "1407.0033: We study low-energy effective field theories for non-Fermi liquids with Fermi su...\n",
      "processed 9999 abstracts\n",
      "\n",
      "abstracts-2small.json:\n",
      "1407.0001: The topic of finding effective strategy to halt virus in complex network is of c...\n",
      "1407.0004: Linear precoding exploits the spatial degrees of freedom offered by multi-antenn...\n",
      "1407.0016: We present completed observations of the NGC 7448 galaxy group and background vo...\n",
      "1407.0017: Increasingly stringent limits from LHC searches for new physics, coupled with la...\n",
      "1407.0023: Results are presented for an initial survey of the Norma Arm gathered with the f...\n",
      "1407.0026: Current time-domain wide-field sky surveys generally operate with few-degree-siz...\n",
      "1407.0029: Spinless fermions on a honeycomb lattice provide a minimal realization of lattic...\n",
      "1407.0030: \\vskip 3pt \\noindent The strong CP-violating parameter is small today as indicat...\n",
      "1407.0031: Cross-correlating the Planck High Frequency Instrument (HFI) maps against quasar...\n",
      "1407.0033: We study low-energy effective field theories for non-Fermi liquids with Fermi su...\n",
      "processed 9999 abstracts\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import codecs\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "files = [\"abstracts-small.json\", \"abstracts-2small.json\"]\n",
    "charvoc = Counter()\n",
    "\n",
    "for file in files:\n",
    "    with open(\"./data/%s\" % file, 'r') as fp:\n",
    "        abstract = json.load(fp)\n",
    "        nabs = 0\n",
    "        print(\"\\n%s:\" % file)\n",
    "        for arxiv_id in abstract:\n",
    "            if nabs < 10:\n",
    "                print(\"%s: %s...\" % (arxiv_id, abstract[arxiv_id][0:80]))\n",
    "            nabs = nabs + 1\n",
    "            charvoc.update([c for c in abstract[arxiv_id].strip()])\n",
    "        print(\"processed %d abstracts\" % nabs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab tokens:  [' ', 'e', 't', 'i', 'a', 'o', 'n', 's', 'r', 'l', 'c', 'h', 'd', 'm', 'u', 'p', 'f', 'g', 'y', 'b', 'w', 'v', '.', '$', ',', '-', '\\\\', '}', '{', 'x', 'k', 'T', 'q', ')', '(', 'W', '1', 'S', '0', 'z', 'I', '2', 'A', 'M', 'C', '_', 'H', 'F', 'L', 'B', 'P', 'D', '^', 'R', 'G', 'N', '3', 'O', 'E', 'j', '5', \"'\", '4', 'K', 'V', '/', 'U', '=', '6', ':', '~', '8', '7', '9', '`', 'X', '+', ';', 'J', 'Q', 'Ã', 'Â', 'Z', ']', '[', 'Y', '\"', '%', '<', '|', '*', '>', '\\x90', '?', '\\x91', '!', '©', '¢', '&']\n",
      "vocab length:  99\n"
     ]
    }
   ],
   "source": [
    "vocab_words = [c[0] for c in charvoc.most_common() if c[1] > 200]\n",
    "print(\"vocab tokens: \", vocab_words)\n",
    "print(\"vocab length: \", len(vocab_words))\n",
    "\n",
    "vocab_words += ['<sos>', '<eos>', '<unk>']\n",
    "vocab = {x: i+1 for i,x in enumerate(vocab_words)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "with open('vocab.pkl', 'wb') as fp:\n",
    "    pickle.dump([vocab, vocab_size], fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'a',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " 'r',\n",
       " 'l',\n",
       " 'c',\n",
       " 'h',\n",
       " 'd',\n",
       " 'm',\n",
       " 'u',\n",
       " 'p',\n",
       " 'f',\n",
       " 'g',\n",
       " 'y',\n",
       " 'b',\n",
       " 'w',\n",
       " 'v',\n",
       " '.',\n",
       " '$',\n",
       " ',',\n",
       " '-',\n",
       " '\\\\',\n",
       " '}',\n",
       " '{',\n",
       " 'x',\n",
       " 'k',\n",
       " 'T',\n",
       " 'q',\n",
       " ')',\n",
       " '(',\n",
       " 'W',\n",
       " '1',\n",
       " 'S',\n",
       " '0',\n",
       " 'z',\n",
       " 'I',\n",
       " '2',\n",
       " 'A',\n",
       " 'M',\n",
       " 'C',\n",
       " '_',\n",
       " 'H',\n",
       " 'F',\n",
       " 'L',\n",
       " 'B',\n",
       " 'P',\n",
       " 'D',\n",
       " '^',\n",
       " 'R',\n",
       " 'G',\n",
       " 'N',\n",
       " '3',\n",
       " 'O',\n",
       " 'E',\n",
       " 'j',\n",
       " '5',\n",
       " \"'\",\n",
       " '4',\n",
       " 'K',\n",
       " 'V',\n",
       " '/',\n",
       " 'U',\n",
       " '=',\n",
       " '6',\n",
       " ':',\n",
       " '~',\n",
       " '8',\n",
       " '7',\n",
       " '9',\n",
       " '`',\n",
       " 'X',\n",
       " '+',\n",
       " ';',\n",
       " 'J',\n",
       " 'Q',\n",
       " 'Ã',\n",
       " 'Â',\n",
       " 'Z',\n",
       " ']',\n",
       " '[',\n",
       " 'Y',\n",
       " '\"',\n",
       " '%',\n",
       " '<',\n",
       " '|',\n",
       " '*',\n",
       " '>',\n",
       " '\\x90',\n",
       " '?',\n",
       " '\\x91',\n",
       " '!',\n",
       " '©',\n",
       " '¢',\n",
       " '&',\n",
       " '\\x83',\n",
       " '¥',\n",
       " '\\x80',\n",
       " '\\xad',\n",
       " '®',\n",
       " '¾',\n",
       " '°',\n",
       " '¸',\n",
       " '½',\n",
       " '@',\n",
       " '¡',\n",
       " 'µ',\n",
       " '¨',\n",
       " '«']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
